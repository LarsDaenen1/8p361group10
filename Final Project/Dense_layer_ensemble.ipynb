{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6780f343",
   "metadata": {},
   "source": [
    "#### Import everthing needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25975ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow and sklearn stuff\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}   \n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import keras\n",
    "from keras.layers import *\n",
    "#import models\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet152V2, preprocess_input\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB5\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0657e97",
   "metadata": {},
   "source": [
    "#### function for training in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6cbb7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcam_generators(base_dir, train_batch_size=32, val_batch_size=32):\n",
    "\n",
    "     # dataset parameters\n",
    "     train_path = os.path.join(base_dir, 'train+val', 'train')\n",
    "     valid_path = os.path.join(base_dir, 'train+val', 'valid')\n",
    "\n",
    "     # instantiate data generators\n",
    "     datagen = ImageDataGenerator(zca_whitening=False,\n",
    "                             horizontal_flip=True,\n",
    "                             vertical_flip=True)\n",
    "\n",
    "     train_gen = datagen.flow_from_directory(train_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=train_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     val_gen = datagen.flow_from_directory(valid_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=val_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     return train_gen, val_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b652c6c5",
   "metadata": {},
   "source": [
    "#### define image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8b6725",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 96\n",
    "\n",
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "model_input = Input(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b005b9",
   "metadata": {},
   "source": [
    "## Load in models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18020914",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_Efficient = r'C:\\Users\\20192653\\OneDrive - TU Eindhoven\\Year 3\\Q3\\8P361 - Project imaging\\Github\\8p361group10\\Final Project\\EfficientNet_weights.hdf5'\n",
    "weights_Xception = r'C:\\Users\\20192653\\OneDrive - TU Eindhoven\\Year 3\\Q3\\8P361 - Project imaging\\Github\\8p361group10\\Final Project\\Xception_weights.hdf5'\n",
    "weights_Inception = r'C:\\Users\\20192653\\OneDrive - TU Eindhoven\\Year 3\\Q3\\8P361 - Project imaging\\Github\\8p361group10\\Final Project\\Inception_weights.hdf5'\n",
    "weights_ResNet =r'C:\\Users\\20192653\\OneDrive - TU Eindhoven\\Year 3\\Q3\\8P361 - Project imaging\\Github\\8p361group10\\Final Project\\ResNet_weights.hdf5'\n",
    "weights_MobileNetV2 = r'C:\\Users\\20192653\\OneDrive - TU Eindhoven\\Year 3\\Q3\\8P361 - Project imaging\\Github\\8p361group10\\Final Project\\MobileNetV2_weights.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5768c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Efficient =tf.keras.models.load_model(weights_Efficient)\n",
    "model_Efficient._name='Efficient'\n",
    "model_Xception =tf.keras.models.load_model(weights_Xception)\n",
    "model_Xception._name = 'Xception'\n",
    "model_Inception =tf.keras.models.load_model(weights_Inception)\n",
    "model_Inception._name = 'Inception'\n",
    "model_ResNet= tf.keras.models.load_model(weights_ResNet)\n",
    "model_ResNet._name = 'ResNet'\n",
    "model_MobileNetV2= tf.keras.models.load_model(weights_MobileNetV2)\n",
    "model_MobileNetV2._name = 'Mobile'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb6634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_Efficient, model_Xception, model_Inception, model_ResNet, model_MobileNetV2]\n",
    "for i in models:\n",
    "    i.trainable= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.RandomUniform(minval=0.2, maxval=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b6d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = [i(model_input) for i in models]\n",
    "concatenated = tf.keras.layers.Dense(1, activation='relu', kernel_initializer=initializer)(concatenated)\n",
    "\n",
    "#merged = keras.layers.Add()(outs)\n",
    "Ensemble = Model(model_input, concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3117bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble._name = 'Ensemble'\n",
    "Ensemble.compile(Adam(learning_rate=0.0001), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "Ensemble.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c8806",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble.layers[-1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d1c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(Ensemble, to_file='Ensemble.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b131ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data generators\n",
    "train_gen, val_gen = get_pcam_generators(r'C:\\Users\\20192653\\Documents\\8P361 - Project imaging\\8p361-project-imaging-master\\8p361-project-imaging-master\\data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028ae245",
   "metadata": {},
   "source": [
    "#### saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c65ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_name = 'Ensemble_model'\n",
    "ensemble_filepath = ensemble_name + '.json'\n",
    "ensemble_weights_filepath = ensemble_name + '_weights.hdf5'\n",
    "\n",
    "ensemble_json = Ensemble.to_json() # serialize model to JSON\n",
    "with open(ensemble_filepath, 'w') as json_file:\n",
    "    json_file.write(ensemble_json)\n",
    "    \n",
    "# define the model checkpoint and Tensorboard callbacks\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=10, verbose=1), tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.000001, verbose=1)]\n",
    "#callbacks = [tf.keras.callbacks.EarlyStopping(patience=10, verbose=1), tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.000001, verbose=1)]\n",
    "checkpoint = ModelCheckpoint(ensemble_weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join('logs', ensemble_name))\n",
    "callbacks_list = [checkpoint, tensorboard] + callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd5a5b",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52472f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = train_gen.n//train_gen.batch_size//20\n",
    "val_steps = val_gen.n//val_gen.batch_size//20\n",
    "\n",
    "history = Ensemble.fit(train_gen, steps_per_epoch=train_steps,\n",
    "                    validation_steps=val_steps,validation_data=val_gen,\n",
    "                    epochs=20,\n",
    "                    callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c19c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble.layers[-1].get_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
