{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "354669eb",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "Run the example in transfer.py. Then, modify the code so that the MobileNetV2 model is not initialized from the ImageNet weights, but randomly (you can do that by setting the weights parameter to None). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f77056",
   "metadata": {},
   "source": [
    "### Run with imagenet weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fb6c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable overly verbose tensorflow logging\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}   \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f49b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcam_generators(base_dir, train_batch_size=32, val_batch_size=32):\n",
    "\n",
    "     # dataset parameters\n",
    "     train_path = os.path.join(base_dir, 'train+val', 'train')\n",
    "     valid_path = os.path.join(base_dir, 'train+val', 'valid')\n",
    "\t \n",
    "     # instantiate data generators\n",
    "     datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "     train_gen = datagen.flow_from_directory(train_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=train_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     val_gen = datagen.flow_from_directory(valid_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=val_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     return train_gen, val_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63bc48e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the size of the images in the PCAM dataset\n",
    "IMAGE_SIZE = 96\n",
    "\n",
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "\n",
    "input = Input(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ec7ad5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " mobilenetv2_1.00_96 (Functi  (None, 3, 3, 1280)       2257984   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " global_average_pooling2d_2   (None, 1280)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 1281      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,259,265\n",
      "Trainable params: 2,225,153\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get the pretrained model, cut out the top layer\n",
    "pretrained = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "\n",
    "# if the pretrained model it to be used as a feature extractor, and not for\n",
    "# fine-tuning, the weights of the model can be frozen in the following way\n",
    "# for layer in pretrained.layers:\n",
    "#    layer.trainable = False\n",
    "\n",
    "output = pretrained(input)\n",
    "output = GlobalAveragePooling2D()(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "model = Model(input, output)\n",
    "\n",
    "# note the lower lr compared to the cnn example\n",
    "model.compile(SGD(learning_rate=0.001, momentum=0.95), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# print a summary of the model on screen\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8b0452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.4613 - accuracy: 0.8049\n",
      "Epoch 1: val_loss improved from inf to 2.36042, saving model to my_first_transfer_model_weights.hdf5\n",
      "225/225 [==============================] - 148s 641ms/step - loss: 0.4613 - accuracy: 0.8049 - val_loss: 2.3604 - val_accuracy: 0.4825\n",
      "Epoch 2/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.3475 - accuracy: 0.8575\n",
      "Epoch 2: val_loss did not improve from 2.36042\n",
      "225/225 [==============================] - 142s 630ms/step - loss: 0.3475 - accuracy: 0.8575 - val_loss: 3.0185 - val_accuracy: 0.5300\n",
      "Epoch 3/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2988 - accuracy: 0.8760\n",
      "Epoch 3: val_loss improved from 2.36042 to 0.99772, saving model to my_first_transfer_model_weights.hdf5\n",
      "225/225 [==============================] - 149s 662ms/step - loss: 0.2988 - accuracy: 0.8760 - val_loss: 0.9977 - val_accuracy: 0.7538\n",
      "Epoch 4/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2756 - accuracy: 0.8871\n",
      "Epoch 4: val_loss improved from 0.99772 to 0.40459, saving model to my_first_transfer_model_weights.hdf5\n",
      "225/225 [==============================] - 154s 686ms/step - loss: 0.2756 - accuracy: 0.8871 - val_loss: 0.4046 - val_accuracy: 0.8363\n",
      "Epoch 5/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2573 - accuracy: 0.8940\n",
      "Epoch 5: val_loss did not improve from 0.40459\n",
      "225/225 [==============================] - 138s 612ms/step - loss: 0.2573 - accuracy: 0.8940 - val_loss: 0.8346 - val_accuracy: 0.7725\n",
      "Epoch 6/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.9022\n",
      "Epoch 6: val_loss did not improve from 0.40459\n",
      "225/225 [==============================] - 134s 595ms/step - loss: 0.2466 - accuracy: 0.9022 - val_loss: 0.6160 - val_accuracy: 0.7738\n",
      "Epoch 7/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2291 - accuracy: 0.9119\n",
      "Epoch 7: val_loss did not improve from 0.40459\n",
      "225/225 [==============================] - 144s 639ms/step - loss: 0.2291 - accuracy: 0.9119 - val_loss: 0.6857 - val_accuracy: 0.7825\n",
      "Epoch 8/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2303 - accuracy: 0.9097\n",
      "Epoch 8: val_loss did not improve from 0.40459\n",
      "225/225 [==============================] - 160s 711ms/step - loss: 0.2303 - accuracy: 0.9097 - val_loss: 0.6830 - val_accuracy: 0.7987\n",
      "Epoch 9/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2169 - accuracy: 0.9135\n",
      "Epoch 9: val_loss improved from 0.40459 to 0.33520, saving model to my_first_transfer_model_weights.hdf5\n",
      "225/225 [==============================] - 136s 606ms/step - loss: 0.2169 - accuracy: 0.9135 - val_loss: 0.3352 - val_accuracy: 0.8838\n",
      "Epoch 10/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2153 - accuracy: 0.9189\n",
      "Epoch 10: val_loss improved from 0.33520 to 0.27567, saving model to my_first_transfer_model_weights.hdf5\n",
      "225/225 [==============================] - 141s 626ms/step - loss: 0.2153 - accuracy: 0.9189 - val_loss: 0.2757 - val_accuracy: 0.9062\n"
     ]
    }
   ],
   "source": [
    "# get the data generators\n",
    "train_gen, val_gen = get_pcam_generators(r'C:\\Users\\20192236\\Documents\\Project_Imaging')\n",
    "\n",
    "\n",
    "# save the model and weights\n",
    "model_name = 'my_first_transfer_model'\n",
    "model_filepath = model_name + '.json'\n",
    "weights_filepath = model_name + '_weights.hdf5'\n",
    "\n",
    "model_json = model.to_json() # serialize model to JSON\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "\n",
    "# define the model checkpoint and Tensorboard callbacks\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join('logs', model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "\n",
    "# train the model, note that we define \"mini-epochs\"\n",
    "train_steps = train_gen.n//train_gen.batch_size//20\n",
    "val_steps = val_gen.n//val_gen.batch_size//20\n",
    "\n",
    "# since the model is trained for only 10 \"mini-epochs\", i.e. half of the data is\n",
    "# not used during training\n",
    "history = model.fit(train_gen, steps_per_epoch=train_steps,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=10,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344560a7",
   "metadata": {},
   "source": [
    "### Run without imagenet weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a100cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable overly verbose tensorflow logging\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}   \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fd4687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcam_generators(base_dir, train_batch_size=32, val_batch_size=32):\n",
    "\n",
    "     # dataset parameters\n",
    "     train_path = os.path.join(base_dir, 'train+val', 'train')\n",
    "     valid_path = os.path.join(base_dir, 'train+val', 'valid')\n",
    "\t \n",
    "     # instantiate data generators\n",
    "     datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "     train_gen = datagen.flow_from_directory(train_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=train_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     val_gen = datagen.flow_from_directory(valid_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=val_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     return train_gen, val_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c018aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the size of the images in the PCAM dataset\n",
    "IMAGE_SIZE = 96\n",
    "\n",
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "\n",
    "input = Input(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f398f4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " mobilenetv2_1.00_96 (Functi  (None, 3, 3, 1280)       2257984   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " global_average_pooling2d_3   (None, 1280)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 1281      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,259,265\n",
      "Trainable params: 2,225,153\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get the pretrained model, cut out the top layer\n",
    "pretrained = MobileNetV2(input_shape=input_shape, include_top=False, weights=None)\n",
    "\n",
    "# if the pretrained model it to be used as a feature extractor, and not for\n",
    "# fine-tuning, the weights of the model can be frozen in the following way\n",
    "# for layer in pretrained.layers:\n",
    "#    layer.trainable = False\n",
    "\n",
    "output = pretrained(input)\n",
    "output = GlobalAveragePooling2D()(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "model = Model(input, output)\n",
    "\n",
    "# note the lower lr compared to the cnn example\n",
    "model.compile(SGD(learning_rate=0.001, momentum=0.95), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# print a summary of the model on screen\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b406781a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.5917 - accuracy: 0.7054\n",
      "Epoch 1: val_loss improved from inf to 0.69436, saving model to Weights_None_transfer_model_weights.hdf5\n",
      "225/225 [==============================] - 134s 583ms/step - loss: 0.5917 - accuracy: 0.7054 - val_loss: 0.6944 - val_accuracy: 0.4863\n",
      "Epoch 2/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.5211 - accuracy: 0.7586\n",
      "Epoch 2: val_loss did not improve from 0.69436\n",
      "225/225 [==============================] - 130s 575ms/step - loss: 0.5211 - accuracy: 0.7586 - val_loss: 0.6944 - val_accuracy: 0.4963\n",
      "Epoch 3/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.4985 - accuracy: 0.7703\n",
      "Epoch 3: val_loss improved from 0.69436 to 0.69311, saving model to Weights_None_transfer_model_weights.hdf5\n",
      "225/225 [==============================] - 130s 578ms/step - loss: 0.4985 - accuracy: 0.7703 - val_loss: 0.6931 - val_accuracy: 0.5063\n",
      "Epoch 4/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.4719 - accuracy: 0.7794\n",
      "Epoch 4: val_loss did not improve from 0.69311\n",
      "225/225 [==============================] - 129s 574ms/step - loss: 0.4719 - accuracy: 0.7794 - val_loss: 0.6941 - val_accuracy: 0.5025\n",
      "Epoch 5/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.4643 - accuracy: 0.7862\n",
      "Epoch 5: val_loss did not improve from 0.69311\n",
      "225/225 [==============================] - 131s 581ms/step - loss: 0.4643 - accuracy: 0.7862 - val_loss: 0.6936 - val_accuracy: 0.4800\n",
      "Epoch 6/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.4147 - accuracy: 0.8147\n",
      "Epoch 6: val_loss improved from 0.69311 to 0.69303, saving model to Weights_None_transfer_model_weights.hdf5\n",
      "225/225 [==============================] - 130s 575ms/step - loss: 0.4147 - accuracy: 0.8147 - val_loss: 0.6930 - val_accuracy: 0.5088\n",
      "Epoch 7/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.4144 - accuracy: 0.8232\n",
      "Epoch 7: val_loss did not improve from 0.69303\n",
      "225/225 [==============================] - 129s 575ms/step - loss: 0.4144 - accuracy: 0.8232 - val_loss: 0.7109 - val_accuracy: 0.4725\n",
      "Epoch 8/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.3942 - accuracy: 0.8296\n",
      "Epoch 8: val_loss did not improve from 0.69303\n",
      "225/225 [==============================] - 129s 574ms/step - loss: 0.3942 - accuracy: 0.8296 - val_loss: 0.6949 - val_accuracy: 0.4812\n",
      "Epoch 9/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.4073 - accuracy: 0.8169\n",
      "Epoch 9: val_loss did not improve from 0.69303\n",
      "225/225 [==============================] - 133s 591ms/step - loss: 0.4073 - accuracy: 0.8169 - val_loss: 0.6945 - val_accuracy: 0.4825\n",
      "Epoch 10/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.3969 - accuracy: 0.8214\n",
      "Epoch 10: val_loss did not improve from 0.69303\n",
      "225/225 [==============================] - 134s 597ms/step - loss: 0.3969 - accuracy: 0.8214 - val_loss: 0.6932 - val_accuracy: 0.5025\n"
     ]
    }
   ],
   "source": [
    "# get the data generators\n",
    "train_gen, val_gen = get_pcam_generators(r'C:\\Users\\20192236\\Documents\\Project_Imaging')\n",
    "\n",
    "\n",
    "# save the model and weights\n",
    "model_name = 'Weights_None_transfer_model'\n",
    "model_filepath = model_name + '.json'\n",
    "weights_filepath = model_name + '_weights.hdf5'\n",
    "\n",
    "model_json = model.to_json() # serialize model to JSON\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "\n",
    "# define the model checkpoint and Tensorboard callbacks\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join('logs', model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "\n",
    "# train the model, note that we define \"mini-epochs\"\n",
    "train_steps = train_gen.n//train_gen.batch_size//20\n",
    "val_steps = val_gen.n//val_gen.batch_size//20\n",
    "\n",
    "# since the model is trained for only 10 \"mini-epochs\", i.e. half of the data is\n",
    "# not used during training\n",
    "history = model.fit(train_gen, steps_per_epoch=train_steps,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=10,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1df768",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "The model in transfer.py uses a dropout layer. How does dropout work and what is the effect of adding dropout layers the the network architecture? What is the observed effect when removing the dropout layer from this model? Hint: check out the Keras documentation for this layer.\n",
    "\n",
    "See pdf for answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a014673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " mobilenetv2_1.00_96 (Functi  (None, 3, 3, 1280)       2257984   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 1281      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,259,265\n",
      "Trainable params: 2,225,153\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\larsd\\anaconda3\\envs\\8p361\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\larsd\\AppData\\Local\\Temp/ipykernel_23368/1850835777.py:92: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_gen, steps_per_epoch=train_steps,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.4184 - accuracy: 0.8106\n",
      "Epoch 1: val_loss improved from inf to 1.77134, saving model to Model_no_dropout_weights.hdf5\n",
      "225/225 [==============================] - 87s 374ms/step - loss: 0.4184 - accuracy: 0.8106 - val_loss: 1.7713 - val_accuracy: 0.5063\n",
      "Epoch 2/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.3056 - accuracy: 0.8719\n",
      "Epoch 2: val_loss improved from 1.77134 to 1.37005, saving model to Model_no_dropout_weights.hdf5\n",
      "225/225 [==============================] - 83s 369ms/step - loss: 0.3056 - accuracy: 0.8719 - val_loss: 1.3701 - val_accuracy: 0.5175\n",
      "Epoch 3/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2751 - accuracy: 0.8869\n",
      "Epoch 3: val_loss improved from 1.37005 to 1.03514, saving model to Model_no_dropout_weights.hdf5\n",
      "225/225 [==============================] - 83s 368ms/step - loss: 0.2751 - accuracy: 0.8869 - val_loss: 1.0351 - val_accuracy: 0.6625\n",
      "Epoch 4/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2591 - accuracy: 0.8964\n",
      "Epoch 4: val_loss did not improve from 1.03514\n",
      "225/225 [==============================] - 82s 364ms/step - loss: 0.2591 - accuracy: 0.8964 - val_loss: 2.5944 - val_accuracy: 0.5525\n",
      "Epoch 5/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2384 - accuracy: 0.9039\n",
      "Epoch 5: val_loss did not improve from 1.03514\n",
      "225/225 [==============================] - 82s 366ms/step - loss: 0.2384 - accuracy: 0.9039 - val_loss: 1.7249 - val_accuracy: 0.5612\n",
      "Epoch 6/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2304 - accuracy: 0.9094\n",
      "Epoch 6: val_loss improved from 1.03514 to 0.77346, saving model to Model_no_dropout_weights.hdf5\n",
      "225/225 [==============================] - 82s 364ms/step - loss: 0.2304 - accuracy: 0.9094 - val_loss: 0.7735 - val_accuracy: 0.7200\n",
      "Epoch 7/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2339 - accuracy: 0.9065\n",
      "Epoch 7: val_loss did not improve from 0.77346\n",
      "225/225 [==============================] - 82s 363ms/step - loss: 0.2339 - accuracy: 0.9065 - val_loss: 1.1192 - val_accuracy: 0.6187\n",
      "Epoch 8/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2252 - accuracy: 0.9119\n",
      "Epoch 8: val_loss improved from 0.77346 to 0.50682, saving model to Model_no_dropout_weights.hdf5\n",
      "225/225 [==============================] - 82s 362ms/step - loss: 0.2252 - accuracy: 0.9119 - val_loss: 0.5068 - val_accuracy: 0.7912\n",
      "Epoch 9/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.1982 - accuracy: 0.9229\n",
      "Epoch 9: val_loss did not improve from 0.50682\n",
      "225/225 [==============================] - 82s 365ms/step - loss: 0.1982 - accuracy: 0.9229 - val_loss: 0.5252 - val_accuracy: 0.7663\n",
      "Epoch 10/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2073 - accuracy: 0.9168\n",
      "Epoch 10: val_loss did not improve from 0.50682\n",
      "225/225 [==============================] - 84s 375ms/step - loss: 0.2073 - accuracy: 0.9168 - val_loss: 0.6336 - val_accuracy: 0.8075\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}   \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "\n",
    "\n",
    "def get_pcam_generators(base_dir, train_batch_size=32, val_batch_size=32):\n",
    "\n",
    "     # dataset parameters\n",
    "     train_path = os.path.join(base_dir, 'train+val', 'train')\n",
    "     valid_path = os.path.join(base_dir, 'train+val', 'valid')\n",
    "\t \n",
    "     # instantiate data generators\n",
    "     datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "     train_gen = datagen.flow_from_directory(train_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=train_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     val_gen = datagen.flow_from_directory(valid_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=val_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     return train_gen, val_gen\n",
    "\n",
    "# the size of the images in the PCAM dataset\n",
    "IMAGE_SIZE = 96\n",
    "\n",
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "\n",
    "input = Input(input_shape)\n",
    "\n",
    "# get the pretrained model, cut out the top layer\n",
    "pretrained = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "#pretrained = MobileNetV2(input_shape=input_shape, include_top=False, weights=None)\n",
    "\n",
    "# if the pretrained model it to be used as a feature extractor, and not for\n",
    "# fine-tuning, the weights of the model can be frozen in the following way\n",
    "# for layer in pretrained.layers:\n",
    "#    layer.trainable = False\n",
    "\n",
    "output = pretrained(input)\n",
    "output = GlobalAveragePooling2D()(output)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "model = Model(input, output)\n",
    "\n",
    "# note the lower lr compared to the cnn example\n",
    "model.compile(SGD(lr=0.001, momentum=0.95), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# print a summary of the model on screen\n",
    "model.summary()\n",
    "\n",
    "# get the data generators\n",
    "train_gen, val_gen = get_pcam_generators(r'C:/Data Project Imaging')\n",
    "\n",
    "\n",
    "# save the model and weights\n",
    "model_name = 'Model_no_dropout'\n",
    "model_filepath = model_name + '.json'\n",
    "weights_filepath = model_name + '_weights.hdf5'\n",
    "\n",
    "model_json = model.to_json() # serialize model to JSON\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "\n",
    "# define the model checkpoint and Tensorboard callbacks\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join('logs', model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "\n",
    "# train the model, note that we define \"mini-epochs\"\n",
    "train_steps = train_gen.n//train_gen.batch_size//20\n",
    "val_steps = val_gen.n//val_gen.batch_size//20\n",
    "\n",
    "# since the model is trained for only 10 \"mini-epochs\", i.e. half of the data is\n",
    "# not used during training\n",
    "history = model.fit_generator(train_gen, steps_per_epoch=train_steps,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=10,\n",
    "                    callbacks=callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
