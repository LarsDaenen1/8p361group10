{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9747f4",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Q: When does transfer learning make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7807f338",
   "metadata": {},
   "source": [
    "* A lot of the low level features such as edge detection or curve detection can be trained on big image recognition datasets. \n",
    "    * these low level features should be helpful for the problem you are transferring to.\n",
    "* Knowledge of the sturcture and nature of images might be useful for the radiology diagnosis dataset because the network does not necessarily need a lot of extra data to train on since it already knows basic features.\n",
    "* Transfer learning makes sense when you have a lot of data for the problem where you are transferring from and relatively less data for the problem you are transferring to. \n",
    "* Tasks should have the same input x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a662cc8",
   "metadata": {},
   "source": [
    "Q: Does it make sense to do transfer learning from ImageNet to the Patch-CAMELYON dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17f2d9f",
   "metadata": {},
   "source": [
    "Yes, all the reasons why transfer learning would make sense apply to the transfer from ImageNet to Patch-CAMELYON."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354669eb",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "Run the example in transfer.py. Then, modify the code so that the MobileNetV2 model is not initialized from the ImageNet weights, but randomly (you can do that by setting the weights parameter to None). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f77056",
   "metadata": {},
   "source": [
    "### Run with imagenet weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fb6c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable overly verbose tensorflow logging\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}   \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f49b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcam_generators(base_dir, train_batch_size=32, val_batch_size=32):\n",
    "\n",
    "     # dataset parameters\n",
    "     train_path = os.path.join(base_dir, 'train+val', 'train')\n",
    "     valid_path = os.path.join(base_dir, 'train+val', 'valid')\n",
    "\t \n",
    "     # instantiate data generators\n",
    "     datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "     train_gen = datagen.flow_from_directory(train_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=train_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     val_gen = datagen.flow_from_directory(valid_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=val_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     return train_gen, val_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63bc48e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the size of the images in the PCAM dataset\n",
    "IMAGE_SIZE = 96\n",
    "\n",
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "\n",
    "input = Input(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ec7ad5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " mobilenetv2_1.00_96 (Functi  (None, 3, 3, 1280)       2257984   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " global_average_pooling2d_2   (None, 1280)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 1281      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,259,265\n",
      "Trainable params: 2,225,153\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get the pretrained model, cut out the top layer\n",
    "pretrained = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "\n",
    "# if the pretrained model it to be used as a feature extractor, and not for\n",
    "# fine-tuning, the weights of the model can be frozen in the following way\n",
    "# for layer in pretrained.layers:\n",
    "#    layer.trainable = False\n",
    "\n",
    "output = pretrained(input)\n",
    "output = GlobalAveragePooling2D()(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "model = Model(input, output)\n",
    "\n",
    "# note the lower lr compared to the cnn example\n",
    "model.compile(SGD(learning_rate=0.001, momentum=0.95), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# print a summary of the model on screen\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8b0452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.4613 - accuracy: 0.8049\n",
      "Epoch 1: val_loss improved from inf to 2.36042, saving model to my_first_transfer_model_weights.hdf5\n",
      "225/225 [==============================] - 148s 641ms/step - loss: 0.4613 - accuracy: 0.8049 - val_loss: 2.3604 - val_accuracy: 0.4825\n",
      "Epoch 2/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.3475 - accuracy: 0.8575\n",
      "Epoch 2: val_loss did not improve from 2.36042\n",
      "225/225 [==============================] - 142s 630ms/step - loss: 0.3475 - accuracy: 0.8575 - val_loss: 3.0185 - val_accuracy: 0.5300\n",
      "Epoch 3/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2988 - accuracy: 0.8760\n",
      "Epoch 3: val_loss improved from 2.36042 to 0.99772, saving model to my_first_transfer_model_weights.hdf5\n",
      "225/225 [==============================] - 149s 662ms/step - loss: 0.2988 - accuracy: 0.8760 - val_loss: 0.9977 - val_accuracy: 0.7538\n",
      "Epoch 4/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2756 - accuracy: 0.8871\n",
      "Epoch 4: val_loss improved from 0.99772 to 0.40459, saving model to my_first_transfer_model_weights.hdf5\n",
      "225/225 [==============================] - 154s 686ms/step - loss: 0.2756 - accuracy: 0.8871 - val_loss: 0.4046 - val_accuracy: 0.8363\n",
      "Epoch 5/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2573 - accuracy: 0.8940\n",
      "Epoch 5: val_loss did not improve from 0.40459\n",
      "225/225 [==============================] - 138s 612ms/step - loss: 0.2573 - accuracy: 0.8940 - val_loss: 0.8346 - val_accuracy: 0.7725\n",
      "Epoch 6/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.9022\n",
      "Epoch 6: val_loss did not improve from 0.40459\n",
      "225/225 [==============================] - 134s 595ms/step - loss: 0.2466 - accuracy: 0.9022 - val_loss: 0.6160 - val_accuracy: 0.7738\n",
      "Epoch 7/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2291 - accuracy: 0.9119\n",
      "Epoch 7: val_loss did not improve from 0.40459\n",
      "225/225 [==============================] - 144s 639ms/step - loss: 0.2291 - accuracy: 0.9119 - val_loss: 0.6857 - val_accuracy: 0.7825\n",
      "Epoch 8/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2303 - accuracy: 0.9097\n",
      "Epoch 8: val_loss did not improve from 0.40459\n",
      "225/225 [==============================] - 160s 711ms/step - loss: 0.2303 - accuracy: 0.9097 - val_loss: 0.6830 - val_accuracy: 0.7987\n",
      "Epoch 9/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2169 - accuracy: 0.9135\n",
      "Epoch 9: val_loss improved from 0.40459 to 0.33520, saving model to my_first_transfer_model_weights.hdf5\n",
      "225/225 [==============================] - 136s 606ms/step - loss: 0.2169 - accuracy: 0.9135 - val_loss: 0.3352 - val_accuracy: 0.8838\n",
      "Epoch 10/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.2153 - accuracy: 0.9189\n",
      "Epoch 10: val_loss improved from 0.33520 to 0.27567, saving model to my_first_transfer_model_weights.hdf5\n",
      "225/225 [==============================] - 141s 626ms/step - loss: 0.2153 - accuracy: 0.9189 - val_loss: 0.2757 - val_accuracy: 0.9062\n"
     ]
    }
   ],
   "source": [
    "# get the data generators\n",
    "train_gen, val_gen = get_pcam_generators(r'C:\\Users\\20192236\\Documents\\Project_Imaging')\n",
    "\n",
    "\n",
    "# save the model and weights\n",
    "model_name = 'my_first_transfer_model'\n",
    "model_filepath = model_name + '.json'\n",
    "weights_filepath = model_name + '_weights.hdf5'\n",
    "\n",
    "model_json = model.to_json() # serialize model to JSON\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "\n",
    "# define the model checkpoint and Tensorboard callbacks\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join('logs', model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "\n",
    "# train the model, note that we define \"mini-epochs\"\n",
    "train_steps = train_gen.n//train_gen.batch_size//20\n",
    "val_steps = val_gen.n//val_gen.batch_size//20\n",
    "\n",
    "# since the model is trained for only 10 \"mini-epochs\", i.e. half of the data is\n",
    "# not used during training\n",
    "history = model.fit(train_gen, steps_per_epoch=train_steps,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=10,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344560a7",
   "metadata": {},
   "source": [
    "### Run without imagenet weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a100cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable overly verbose tensorflow logging\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}   \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fd4687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcam_generators(base_dir, train_batch_size=32, val_batch_size=32):\n",
    "\n",
    "     # dataset parameters\n",
    "     train_path = os.path.join(base_dir, 'train+val', 'train')\n",
    "     valid_path = os.path.join(base_dir, 'train+val', 'valid')\n",
    "\t \n",
    "     # instantiate data generators\n",
    "     datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "     train_gen = datagen.flow_from_directory(train_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=train_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     val_gen = datagen.flow_from_directory(valid_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=val_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     return train_gen, val_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c018aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the size of the images in the PCAM dataset\n",
    "IMAGE_SIZE = 96\n",
    "\n",
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "\n",
    "input = Input(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f398f4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " mobilenetv2_1.00_96 (Functi  (None, 3, 3, 1280)       2257984   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " global_average_pooling2d_3   (None, 1280)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 1281      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,259,265\n",
      "Trainable params: 2,225,153\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get the pretrained model, cut out the top layer\n",
    "pretrained = MobileNetV2(input_shape=input_shape, include_top=False, weights=None)\n",
    "\n",
    "# if the pretrained model it to be used as a feature extractor, and not for\n",
    "# fine-tuning, the weights of the model can be frozen in the following way\n",
    "# for layer in pretrained.layers:\n",
    "#    layer.trainable = False\n",
    "\n",
    "output = pretrained(input)\n",
    "output = GlobalAveragePooling2D()(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "model = Model(input, output)\n",
    "\n",
    "# note the lower lr compared to the cnn example\n",
    "model.compile(SGD(learning_rate=0.001, momentum=0.95), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# print a summary of the model on screen\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b406781a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.5917 - accuracy: 0.7054\n",
      "Epoch 1: val_loss improved from inf to 0.69436, saving model to Weights_None_transfer_model_weights.hdf5\n",
      "225/225 [==============================] - 134s 583ms/step - loss: 0.5917 - accuracy: 0.7054 - val_loss: 0.6944 - val_accuracy: 0.4863\n",
      "Epoch 2/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.5211 - accuracy: 0.7586\n",
      "Epoch 2: val_loss did not improve from 0.69436\n",
      "225/225 [==============================] - 130s 575ms/step - loss: 0.5211 - accuracy: 0.7586 - val_loss: 0.6944 - val_accuracy: 0.4963\n",
      "Epoch 3/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.4985 - accuracy: 0.7703\n",
      "Epoch 3: val_loss improved from 0.69436 to 0.69311, saving model to Weights_None_transfer_model_weights.hdf5\n",
      "225/225 [==============================] - 130s 578ms/step - loss: 0.4985 - accuracy: 0.7703 - val_loss: 0.6931 - val_accuracy: 0.5063\n",
      "Epoch 4/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.4719 - accuracy: 0.7794\n",
      "Epoch 4: val_loss did not improve from 0.69311\n",
      "225/225 [==============================] - 129s 574ms/step - loss: 0.4719 - accuracy: 0.7794 - val_loss: 0.6941 - val_accuracy: 0.5025\n",
      "Epoch 5/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.4643 - accuracy: 0.7862\n",
      "Epoch 5: val_loss did not improve from 0.69311\n",
      "225/225 [==============================] - 131s 581ms/step - loss: 0.4643 - accuracy: 0.7862 - val_loss: 0.6936 - val_accuracy: 0.4800\n",
      "Epoch 6/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.4147 - accuracy: 0.8147\n",
      "Epoch 6: val_loss improved from 0.69311 to 0.69303, saving model to Weights_None_transfer_model_weights.hdf5\n",
      "225/225 [==============================] - 130s 575ms/step - loss: 0.4147 - accuracy: 0.8147 - val_loss: 0.6930 - val_accuracy: 0.5088\n",
      "Epoch 7/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.4144 - accuracy: 0.8232\n",
      "Epoch 7: val_loss did not improve from 0.69303\n",
      "225/225 [==============================] - 129s 575ms/step - loss: 0.4144 - accuracy: 0.8232 - val_loss: 0.7109 - val_accuracy: 0.4725\n",
      "Epoch 8/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.3942 - accuracy: 0.8296\n",
      "Epoch 8: val_loss did not improve from 0.69303\n",
      "225/225 [==============================] - 129s 574ms/step - loss: 0.3942 - accuracy: 0.8296 - val_loss: 0.6949 - val_accuracy: 0.4812\n",
      "Epoch 9/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.4073 - accuracy: 0.8169\n",
      "Epoch 9: val_loss did not improve from 0.69303\n",
      "225/225 [==============================] - 133s 591ms/step - loss: 0.4073 - accuracy: 0.8169 - val_loss: 0.6945 - val_accuracy: 0.4825\n",
      "Epoch 10/10\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.3969 - accuracy: 0.8214\n",
      "Epoch 10: val_loss did not improve from 0.69303\n",
      "225/225 [==============================] - 134s 597ms/step - loss: 0.3969 - accuracy: 0.8214 - val_loss: 0.6932 - val_accuracy: 0.5025\n"
     ]
    }
   ],
   "source": [
    "# get the data generators\n",
    "train_gen, val_gen = get_pcam_generators(r'C:\\Users\\20192236\\Documents\\Project_Imaging')\n",
    "\n",
    "\n",
    "# save the model and weights\n",
    "model_name = 'Weights_None_transfer_model'\n",
    "model_filepath = model_name + '.json'\n",
    "weights_filepath = model_name + '_weights.hdf5'\n",
    "\n",
    "model_json = model.to_json() # serialize model to JSON\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "\n",
    "# define the model checkpoint and Tensorboard callbacks\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join('logs', model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "\n",
    "# train the model, note that we define \"mini-epochs\"\n",
    "train_steps = train_gen.n//train_gen.batch_size//20\n",
    "val_steps = val_gen.n//val_gen.batch_size//20\n",
    "\n",
    "# since the model is trained for only 10 \"mini-epochs\", i.e. half of the data is\n",
    "# not used during training\n",
    "history = model.fit(train_gen, steps_per_epoch=train_steps,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=10,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c7fff2",
   "metadata": {},
   "source": [
    "Q: Analyze the results from both runs and compare them to the CNN example in assignment 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5e6063",
   "metadata": {},
   "source": [
    "transfer model with imagenet weights: <br>",
    "val_loss: 0.2757 - val_accuracy: 0.9062 <br>",
    "transfer model without imagenet weights: <br>",
    "val_loss: 0.6932 - val_accuracy: 0.5025 <br>",
    "CNN model from assignment 3: <br>",
    "val_loss: 0.3280 - val_accuracy: 0.8565 <br>",
    "<br>",
    "The most accuracy model is the transfer model with imagenet weights, than the model from assignment 3 and than the transfer model without imagenet weights.\n",
    "This is to be expected since the transfer model with imagenet initialization is trained on a very large dataset and therefore knows all the low level features. The model without the imagenet weights has too little data from the Patch-CAMELYON dataset to train on which results in the weights not converging to an accurate predicting model. The model from assignment 3 has a bit lower accuracy than the transfer model with weights initialization. This could be the result of not having enough data to train on or the slight difference in model structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723ae0ba",
   "metadata": {},
   "source": [
        "## Exercise 3\n",
    "\n",
    "The model in `transfer.py` uses a dropout layer. How does dropout work and what is the effect of adding dropout layers the the network architecture? What is the observed effect when removing the dropout layer from this model? Hint: check out the Keras documentation for this layer.\n",
    "\n",
    "The function of a dropout layer is that inputs are set to 0 randomly. The remaining inputs are scaled up up by 1/(1 - rate). This ensures that the sum over all inputs remains the same. The rate at which this is done can be specified. A dropout layer is used to prevent overfiting. \n",
    "\n",
    "Results without dropout layer:\n",
    "- val_loss: 0.2425 \n",
    "- val_accuracy: 0.8975\n",
    "\n",
    "The validation loss decreases compared to the model with ImageNet weights. The validation accuracy also improves a bit.\n",]
  },
  {
   "cell_type": "markdown",
   "id": "b67e004c",
   "metadata": {},
   "source": [
    " <b>not answered yet</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
